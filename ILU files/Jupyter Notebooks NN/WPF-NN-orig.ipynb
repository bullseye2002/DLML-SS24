{"cells":[{"cell_type":"markdown","metadata":{"id":"j0LP1TMcdewP"},"source":["# Notwendige Bibliotheken/ Required Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DppyqinjdewR","executionInfo":{"status":"ok","timestamp":1652795468575,"user_tz":-120,"elapsed":527,"user":{"displayName":"Wolfgang Konen","userId":"02472779403428507241"}}},"outputs":[],"source":["import matplotlib.pyplot as plt \n","import numpy as np              \n","import collections\n","import pdb   # This library is used for debugging the code, otherwise it is not necessary. \n","             # You can simply enter pdb.set_trace() somewhere in your code for debugging purposes.\n","import copy  # You need this in order to copy lists containing matrices "]},{"cell_type":"markdown","metadata":{"id":"CrrbHzzHdewS"},"source":["# Perceptron\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DuXGBzNcdewT","colab":{"base_uri":"https://localhost:8080/","height":132},"executionInfo":{"status":"error","timestamp":1652788327221,"user_tz":-120,"elapsed":249,"user":{"displayName":"Wolfgang Konen","userId":"02472779403428507241"}},"outputId":"4e4ed475-639b-42a5-ec6c-56f13f083809"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-05d5e4ad8441>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    sigm = ???\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# Define a function that calculates sigmoid(x)\n","# If x is a scalar, return a scalar.\n","# If x is a vector, return a vector.\n","def sigmoid(x):\n","    sigm = ???\n","    return(sigm)\n","\n","# Define a function for a perceptron which gets the input vector and parameter \n","# vector of the perceptron as input and returns a perceptron's output\n","def perceptron(x,theta):\n","    # Don't forget to add the bias to your input vector \n","    x=np.append(1,x)\n","    # With help of matrix multiplication (don't use any for-loop here) determine \n","    # the weighted sum of the input values\n","    z= ???\n","    # Apply the sigmoid activation function on the weighted sum z\n","    a= ???\n","    return(a)"]},{"cell_type":"markdown","metadata":{"id":"YimZNRjOdewU"},"source":["# AND Gate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L55O8_O7dewU"},"outputs":[],"source":["# Did you find a proper set of parameters for the AND gate? Formulate them as \n","# a vector with the numpy array theta\n","theta=np.array([?,?,?])\n","# Now it's time to test your code.\n","# Call your perceptron function for different input values\n","# Check if your model is simulating the AND Gate reasonably well\n","X=np.array([[0,0],[0,1],[1,0],[1,1]])\n","[print(\"[X=\",X[i,:],\"]:: Perceptron returns:\",perceptron(X[i,:],theta),\", Real value is:\",np.logical_and(X[i,0],X[i,1])*1) for i in range(4)];"]},{"cell_type":"markdown","metadata":{"id":"ntos5bNZdewV"},"source":["# OR Gate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG9NsSt2dewV"},"outputs":[],"source":["theta=np.array([0,0,0])\n","[print(\"[X=\",X[i,:],\"]:: Perceptron returns:\",perceptron(X[i,:],theta),\", Real value is:\",np.logical_or(X[i,0],X[i,1])*1) for i in range(4)];"]},{"cell_type":"markdown","metadata":{"id":"sg-S1aSSdewW"},"source":["# Feedforward \n","You should implement the feedforward function for a network with an arbitrary number of hidden layers and an arbitrary number of neurons in each hidden layer. To do so you need to first initialize your parameters. At this moment don't worry about the initialization step as it is done for you. Instead try to complete the feedforward function and test your implementation.\n"]},{"cell_type":"markdown","metadata":{"id":"2v-8oXDEdewX"},"source":["## Initialization\n","$\\color{red}{Task:}$ To test if everything is working as desired, initialize the parameters for the smallest 1-hidden-layer neural network that you know with an input dimension of 3. Check the shapes and contents of the parameter matrix list. Are they as you expected?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HTwqQIWdewX"},"outputs":[],"source":["# The initialize(neurons) function gets the number of neurons as a list of integers called neurons. \n","# The first value in the list is the input dimension. \n","# The last value is the number of outputs which should be set to 1 in case of binary classification.\n","# This function returns a list of parameters containing L-1 theta matrices. \n","# This function realizes uniform initialization in range [-initEps,+initEps].\n","def initialize(neurons,initEps=1,type='uniform',seed=42):\n","    np.random.seed(seed)\n","    L=len(neurons)\n","    Theta=[0]*(L-1)\n","    for i in range(L-1):\n","        Theta[i]=(2*initEps)*np.random.rand((neurons[i]+1),neurons[i+1])-initEps\n","    return Theta \n","\n","# Check the initialize function\n","neurons=[?,?]\n","Theta=initialize(neurons)\n","print(???)"]},{"cell_type":"markdown","metadata":{"id":"J2X7W154dewY"},"source":["## Feedforward\n","Program the feedforward function in a way that it receives a single entry (example) x and a list of initialized parameters and returns a list of all the activation vectors of the neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COilICZ9dewY"},"outputs":[],"source":["# This function feedforwards one single entry\n","def feedforward(x,Theta):\n","    # First find the number of layers\n","    L=len(Theta)+1\n","    # Make an empty list of size L for the activation values\n","    A=[None]*(L)\n","    # Initialize the first layer which is the input\n","    A[0]=???\n","    for i in range(L-1):\n","        A[i]=A[i].reshape(-1,1)      \n","        A[i+1]=???\n","    return A  "]},{"cell_type":"markdown","metadata":{"id":"dLVeaETNdewZ"},"source":["$\\color{red}{Task:}$ With help of function `initialize`, initialize the parameters for a neural network with an input layer of size 3, 5 hidden layers with 8 hidden neurons each and an output layer with one neuron. Then take x=np.array([1,2,3])  and feed it to your neural network. If you have programmed everything correctly then the network should output [0.7569]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1SyE-IbdewZ"},"outputs":[],"source":["# How should the neurons list look like?\n","neurons=???\n","# Initialize the parameters using the already programmed initialize function\n","Theta=???\n","# Define the input vector x\n","x=np.array([1,2,3])\n","# Feed x to the network\n","A=???\n","# Print the output of the network\n","print(???)"]},{"cell_type":"markdown","metadata":{"id":"kj-15drqdewZ"},"source":["$\\color{red}{Task:}$ Taking a suitable set of parameters, can you model AND & OR Gates?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Uz2DzWTdewa"},"outputs":[],"source":["# Define a perceptron with a 2-d input\n","neurons=??\n","# Choose the proper parameter vector\n","Theta=[???]\n","# Test the network for different examples\n","X=np.array([[0,0],[0,1],[1,0],[1,1]])\n","[print(\"[X=\",X[i,:],\"]:: Feedforward returns:\",feedforward(X[i,:],Theta)[-1],\", Real value is:\",np.logical_and(X[i,0],X[i,1])*1) for i in range(4)];"]},{"cell_type":"markdown","metadata":{"id":"GAEe49Y8dewa"},"source":["# Loss Function\n","In this section we program the cross-entropy loss function. Define the loss function in two different forms.\n","\n"," 1. loss(A,Y) : a function that takes the output of the network (the activation in the last layer) and the real expected value (label) to calculate the loss. This would be the direct translation of the loss formula:\n"," \\begin{equation}\n"," J_k(\\mathbf{\\Theta})=-(\\mathbf{y}_k log(\\mathbf{a}_k^{(L-1)})+(1-\\mathbf{y}_k)log(1-\\mathbf{a}_k^{(L-1)})),\n"," \\end{equation}\n","\n"," \n"," 2. lossTheta(x,y,Theta): a function that computes the loss function. It first determines the activation values and the output of a specific neural network with the given parameters Theta for a given input x.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"L0Lf1N7Hdewb","colab":{"base_uri":"https://localhost:8080/","height":132},"executionInfo":{"status":"error","timestamp":1652801685484,"user_tz":-120,"elapsed":211,"user":{"displayName":"Wolfgang Konen","userId":"02472779403428507241"}},"outputId":"9e2706b9-6b83-4065-c4f5-c4e17c721b38"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-abd6d74e2593>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    A=???\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["def loss(A,Y):\n","    errorVec=Y*np.log(np.maximum(A,1e-50))+(1-Y)*np.log(np.maximum(1-A,1e-50))\n","    error=np.sum(-errorVec)\n","    return error \n","\n","\n","def lossTheta(x,y,Theta):\n","    # Feed the entry x to the neural network with the parameters Theta\n","    A=???\n","    # Call the loss function\n","    error=???\n","    return(error)"]},{"cell_type":"code","source":["lossTheta(np.array([1,2,3]),1,Theta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"id":"sXZUm4M6eBVZ","executionInfo":{"status":"error","timestamp":1652801681057,"user_tz":-120,"elapsed":244,"user":{"displayName":"Wolfgang Konen","userId":"02472779403428507241"}},"outputId":"f004cb47-5e14-4d2e-9cc9-37bf9cb4e511"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-96bae226c9b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlossTheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'lossTheta' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"3gFy96yNdewb"},"source":["$\\color{red}{Task :}$ Assume that your output should be 1 but your neural network returns 0.5. What is the loss? What if the network returns 0? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjVmva9rdewb"},"outputs":[],"source":["a=?\n","y=?\n","print(\"The loss is\",loss(a,y), \"if a=\",a,\"and y=\",y)\n","a=?\n","print(\"The loss is\",loss(a,y), \"if a=\",a,\"and y=\",y)"]},{"cell_type":"markdown","metadata":{"id":"QWAf8m6ydewb"},"source":["$\\color{red}{Task :}$ With help of matplotlib show how the loss function changes if the network returns values between 0 and 1 for the case where the true output should be 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdjNx5Dvdewc"},"outputs":[],"source":["a = np.linspace(0, 1, 1000)\n","losses=???\n","plt.plot(a, losses);"]},{"cell_type":"markdown","metadata":{"id":"ZUU6Gznndewc"},"source":["$\\color{red}{Task :}$ With help of matplotlib show how the loss function changes if the network returns values between 0 and 1 for the case where the true output should be 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvxKtrQ_dewd"},"outputs":[],"source":["a = np.linspace(0, 1, 1000)\n","losses=???\n","plt.plot(a, losses);"]},{"cell_type":"markdown","metadata":{"id":"s2vWG2vvdewd"},"source":["# Backpropagation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFaG6gVXdewd"},"outputs":[],"source":["# This function computes the derivatives of the loss function with respect to every single parameter.\n","# This function takes: a list of parameters Theta, containing parameter matrices for each layer, \n","#                     ,a list of activation values A, containing activation vectors for each layer\n","#                     ,y the expected output value\n","# This function returns a list of derivatives Delta: containing the derivative matrices for each layer. \n","# These derivative matrices have the same shape as the parameter matrices\n","def backprop(Theta,A,y):\n","    # Determine the number of layers\n","    L=len(A)\n","    # Initialize the delta vectors with 0\n","    delta=[0]*(L)\n","    # Initialize the Delta matrices with 0\n","    Delta=[0]*(L-1)\n","    # Calculate the delta in the last layer\n","    # delta in the last layer is a vector in general \n","    # but because we restrict ourselves to binary classification, this value is a scalar in our case\n","    delta[L-1]=A[-1] - y\n","    delta[L-1]=delta[L-1].reshape(neurons[L-1],1)\n","    # Determine all other delta vectors and Delta matrices \n","    # by going through a loop starting from layer L-2 going to zero\n","    for i in reversed(range(L-1)):\n","        delta[i]=np.multiply(np.matmul(Theta[i][1:,],delta[i+1]),np.multiply(A[i],1-A[i]))\n","        Delta[i]=np.matmul(np.vstack([1,A[i]]),delta[i+1].T)\n","    return Delta\n"]},{"cell_type":"markdown","metadata":{"id":"nQb5ouYqY21k"},"source":["This astonishing small piece of code does all the backpropagation!\n","It is quite short, but it takes long to understand every detail. It relies on the sigmoid function that we defined above.\n","\n","We take it here as a 'black box' function."]},{"cell_type":"markdown","metadata":{"id":"QbtlDDCYdewe"},"source":["# Gradient Checking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVolTGn7dewe"},"outputs":[],"source":["#Gradient checking\n","def gradientChecking(Theta,x,y,eps=1e-7):\n","    # Copy the list of parameters into a new list called AppDelta which stands for 'Approximative Deltas'\n","    # Or generate a list called AppDelta with the same length and structure as Theta\n","    AppDelta=copy.deepcopy(Theta)\n","    \n","    #loop through all the layer and all the parameters in each layer\n","    for i in range(len(Theta)):\n","        for j in range(Theta[i].shape[0]):\n","            for k in range(Theta[i].shape[1]):\n","                # make two deep copies of the parameter matrix in layer i\n","                # one for the postive side Thetap and one for the negative side Thetan\n","                Thetap=copy.deepcopy(Theta)\n","                Thetan=copy.deepcopy(Theta)\n","                Thetap[i][j,k]=Thetap[i][j,k]+eps\n","                Thetan[i][j,k]=Thetan[i][j,k]-eps\n","                # Store the approximated derivative for \n","                # the parameter value connecting the \n","                # j-th neuron from layer i to the k-th neuron\n","                # in layer i+1\n","                AppDelta[i][j,k]=(lossTheta(x,y,Thetap)-lossTheta(x,y,Thetan))/(2*eps)\n","                \n","    return AppDelta"]},{"cell_type":"markdown","metadata":{"id":"sf5PZUfidewf"},"source":["$\\color{red}{Task :}$ Calculate the derivatives for the above network in both ways: Backpropagation and approximating gradients. Do you get the same results? If you select $\\epsilon$ small enough then your results should become almost identical. Is it the case for you? If not go back and check your code. Can you find the bug? Take a smaller architecture if it is easier to spot the mistakes you have done. Try the same thing for much larger $\\epsilon$ and much smaller $\\epsilon$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBoy2ie6dewf"},"outputs":[],"source":["Delta=???\n","AppDelta=???\n","print(\"Delta:\",Delta)\n","print(\"AppDelta:\",AppDelta)\n","print(\"The difference between Delta and AppDelta:\",[Delta[k]-AppDelta[k] for k in range(len(Delta))])"]},{"cell_type":"markdown","metadata":{"id":"KDKb3ZRSdewf"},"source":["## If numerical derivatives provide more or less same results as backpropagation then why should we use backpropagation at all? Numerical derivative is much easier to understand and program. \n","\n","In order to answer this question you might want to compare the computational time required to determine the derivatives through both approaches. To do so, use %timeit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqb-MvBIdewg"},"outputs":[],"source":["# Compare the computational time here\n"]},{"cell_type":"markdown","metadata":{"id":"tnSPZyvSdewg"},"source":["# Initialization: 4 Methods\n","\n","*--- This part for lecture 'DNN with Keras' ---*\n","\n","Implement four different initialization approaches for any arbitrary NN architecture.\n"," * Zero initialization\n"," * Uniformly random initialization (positive) $\\theta_{ij} \\in U(0,\\epsilon)$\n"," * Uniformly random initialization centered around zero $\\theta_{ij} \\in U(-\\epsilon,\\epsilon)$\n"," * Xavier Glorot initialization $\\theta_{ij} \\in N(0,1)*\\sqrt{\\frac{2}{m_{l-1}}}$ with normal (Gaussian) distribution $N(0,1)$\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BpY8DPedewg"},"outputs":[],"source":["# The initialize(neurons) function gets the number of neurons as a list of integers called neurons. \n","##   The first value in the list neurons is the input dimension. \n","##   The last value is the number of outputs which should be set to 1 in case of binary classification.\n","##   This function returns a list of parameters containing L-1 theta matrices\n","# Argument type can be set to: 'zero', 'uniformp' ,'uniform', 'xavier'\n","# Argument initEps is only relevant if type is set to 'uniform' or 'uniformp': max(|weights|)\n","# This function returns a list containing L-1 theta matrices\n","def initialize(neurons,type='xavier',initEps=1,seed=42):\n","    np.random.seed(seed)\n","    L=len(neurons)\n","    Theta=[0]*(L-1)\n","    for i in range(L-1):\n","        if(type=='xavier'):\n","            Theta[i]=???\n","        if(type=='zero'):\n","            Theta[i]=???\n","        if(type=='uniform'):\n","            Theta[i]=???\n","        if(type=='uniformp'):\n","            Theta[i]=???      \n","    return Theta \n","\n"]},{"cell_type":"markdown","metadata":{"id":"xCbbcNYVdewg"},"source":["$\\color{red}{Task:}$ Check if you have programmed everythig correctly.\n"," * Initialize a network with zeros (neurons=[2,3,5,1]). Print the shape of each parameter matrix. Do they have correct shapes? $\\mathbf{\\theta}^{(0)}_{[3\\times3]}$, $\\mathbf{\\theta}^{(1)}_{[4\\times5]}$, $\\mathbf{\\theta}^{(2)}_{[6\\times1]}$.\n"," * Initialize a network with positive uniform random values between 0 and 1 (neurons=[2,5,5,2,1]). $\\theta^{(3)}_1  =0.04522729$ with default seed?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTOsYAEVdewg"},"outputs":[],"source":["#first part of the task\n","neurons=???\n","#Initialize all the parameters\n","Theta=initialize(neurons,type=??)\n","res=[print(\"Shape of Theta in \",i,\"layer is:\",Theta[i].shape) for i in range(len(Theta))]\n","\n","#second part of the task comes here\n"]},{"cell_type":"markdown","metadata":{"id":"goemyPETdewh"},"source":["# Vanishing Gradient Problem\n","*--- This part for lecture 'DNN with Keras' ---*\n","\n","Consider a feedforward NN with 6 layers (neurons=[2,3,5,3,2,1]). Initialize this network with uniformly random values in different ranges of $[0,1]$, $[-1,1]$, $[-10,10]$ and $[-0.01,0.01]$. What happens if the initialization method is Xavier? Would you recommend using zero initialization? Take a look at the propagated gradients in the first layer $\\mathbf{\\Theta}^{(0)}$ for each case. Can you explain the results? Are the gradients vanishing or exploding? why?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bruzCUhAdewh"},"outputs":[],"source":["#example\n","neurons=[2,3,5,3,2,1]\n","# Initialize all the parameters\n","Theta=???\n","# Define the input and output \n","x=???\n","y=???\n","# Determine the gradients with gradientChecking \n","Delta=gradientChecking(???)\n","res=[print(\"Mean of Delta in layer \",i,\" is:\",np.mean(Delta[i])) for i in range(len(Delta))]\n"]},{"cell_type":"markdown","metadata":{"id":"PLZMqJvBdewh"},"source":["# Let's Put Everything Together (Training A Neural Network)\n","\n","When we feed all the entries of the training dataset to the network and update the parameters then we have completed an **epoch**. In order to train a neural network often (many) more than one epoch is necessary. Therefore, you need to consider a for-loop repeating the neural network training steps ([feedforward (all examples), backpropagate (all examples)], update the parameters) for a fixed number of times which can be set as a parameter called _epochs_. So your trainNN function should have an initialization and two nested for loops. \n","\n","  * initialize network (here you use _seed_)\n","  * for epoch in epochs:\n","    *    for x in data:\n","     *        feedforward\n","     *        backpropagate\n","    *    update the parameters\n","        \n","\n","Choosing the number of necessary epochs to learn a model is not always trivial. This can depend on the problem setting as well as the architecture of the used neural network. You can monitor the cost function or other evaluation measures to decide when to stop.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_HR1UnZdewh"},"outputs":[],"source":["def trainNN(data,neurons,epochs,learningRate,initEps=0.5,initType=\"uniform\",Analyse=True,errorTrace=100,plotTrace=100,seed=42):\n","    \n","    # Determine the number of layers\n","    L=len(neurons)\n","    # Determine the number of examples in the training dataset\n","    K=data.shape[0]\n","    \n","    # Learning rate will be used in  parameter updating phase \n","    # Learning rate is the parameter used in the gradient descent optimization\n","    a=learningRate\n","    \n","    # Initialize the parameters list Theta\n","    Theta=???\n","    \n","    # A for-loop running epochs times\n","    for epoch in range(epochs+1):\n","        # This list is only used for analysis purposes and not the learning\n","        PREDICT=[]\n","        \n","        # In each epoch new derivative should be calculated\n","        accumulator=[0]*len(Theta)\n","        \n","        # Initialize the cost value\n","        totalCost=0\n","        \n","        # Loop through each example in your dataset\n","        for i in range(K):\n","            # Take the i-th example\n","            # Seperate the input and output save them in x and y\n","            x=data[i,:-1]\n","            y=data[i,-1]\n","            # Feedforward and determine the activation vectors\n","            A=???\n","            # Accumulate the loss to have the cost value at the end of this loop \n","            totalCost=???\n","            # Backpropagate to determine the gradient matrices\n","            Delta=???\n","            # Accumulate the gradients\n","            accumulator=[accumulator[l]+Delta[l] for l in range(len(Delta))]\n","\n","            # Saving the instant prediction of the netwrok to asses its performance in each epoch\n","            predict=A[-1]\n","            PREDICT.extend(predict)\n","            Y=data[:,-1]\n","            #End of the for-loop\n","        # Analyse the results of the last epoch  \n","        if(Analyse):\n","            AnalyseNN(PREDICT,Y,totalCost,epoch,errorTrace,plotTrace) \n","        \n","        # Update the parameters    \n","        Theta=[Theta[k]-(a/K)*accumulator[k] for k in range(len(accumulator))]    \n","    return Theta\n","\n","# This function prints the totalCost function in every errorTrace epochs\n","# Also it prints the accuracy of classification on the training data\n","# Also it plots how the data is seperated by the network in every plotTrace epochs\n","# The points with a red circle around are those which are classified wrong\n","def AnalyseNN(PREDICT,Y,totalCost,epoch,errorTrace,plotTrace):\n","    PREDICT = np.asarray(PREDICT)\n","    K=PREDICT.shape[0]\n","    low_values_flags = PREDICT < 0.5  # Where values are low\n","    PREDICT[low_values_flags] = 0 \n","    nonzero_flags = PREDICT !=0\n","    PREDICT[nonzero_flags] = 1\n","    ERROR=np.abs(Y-PREDICT)\n","    accuracy=1-np.sum(ERROR)/K\n","    wrong_value_flags= ERROR>0\n","    WRONGP=data[wrong_value_flags,:]\n","    if (epoch % errorTrace ==0):\n","        print(\"epoch:\",epoch,\", cost = \",totalCost/K)\n","        print(\"epoch:\",epoch,\", accuracy = \",accuracy)\n","    if (epoch % plotTrace ==0):\n","        plt.scatter(WRONGP[:,0], WRONGP[:,1],edgecolor='red',facecolors='none',marker='o',s=100)\n","        plt.scatter(data[:,0], data[:,1],c=PREDICT)\n","        plt.show()\n","    return      "]},{"cell_type":"markdown","metadata":{"id":"0zk-Akiedewi"},"source":["## A Helper Function to Analyse the Input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGAvDc5gdewi"},"outputs":[],"source":["# This function receives the training data and \n","# plots a histogram showing the frequency of the samples in different classes\n","# It also plots a scatter plot of the data (only if the input is 2-dimensional)\n","\n","def AnalyseData(data,PLOT=True):\n","    m=data.shape[0]\n","    dim=data.shape[1]-1\n","    c = collections.Counter(data[:,2]).items()\n","    classes = [i[0] for i in c]\n","    freq = [i[1] for i in c]\n","    if(len(classes)==2):\n","        print(\"A binary classification task:\")\n","    print(\"This data set has \"+str(len(classes))+' classes:'+str(classes))\n","    print(\"The frequency of classes are: \"+str(freq))\n","    if(PLOT):\n","        plt.bar(classes, freq)\n","        plt.title(\"\")\n","        plt.xlabel(\"Classes\")\n","        plt.ylabel(\"Frequency\")\n","        plt.xticks(classes)\n","        plt.show()\n","        if(dim==2):\n","            plt.scatter(data[:,0], data[:,1],c=data[:,2])\n","            plt.show()\n","    return\n"]},{"cell_type":"markdown","metadata":{"id":"wZAxfTUxdewi"},"source":["$\\color{red}{Task:}$ Choose a dataset among: \"admission\", \"pizza\", \"XOR\", \"XORN\", load it first and analyse it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOm87j0-dewj"},"outputs":[],"source":["# Give the Path were your data is stored in variable path\n","path=???\n","# OR, if you are on Google Colab (put the data beforehand on your Google Drive):\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Choose a dataset\n","dataName=???\n","\n","# Load the data\n","mypath=path+dataName+'.csv'\n","# OR, if you are on Google Colab:\n","mypath = \"/content/gdrive/My Drive/data/\"+dataName+\".csv\"\n","\n","data=np.loadtxt(mypath, delimiter=',')\n","\n","AnalyseData(data)"]},{"cell_type":"markdown","metadata":{"id":"aaFVGBDadewj"},"source":["## Train a Neural Network for Your Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cK5y8sxkdewj"},"outputs":[],"source":["# Choose one of the following datasets:\n","# \"admission\", \"pizza\", \"XOR\", \"XORN\"\n","dataName=\"pizza\"\n","\n","# Load the data\n","mypath=path+dataName+'.csv'\n","# OR, if you are on Google Colab:\n","mypath = \"/content/gdrive/My Drive/data/\"+dataName+\".csv\"\n","\n","data=np.loadtxt(mypath, delimiter=',')\n","AnalyseData(data)\n","\n","# Design a desired architecure\n","neurons=???\n","\n","# Choose the learning rate\n","learningRate=0.9\n","\n","# Choose the number of epochs\n","epochs=1000\n","\n","# Train your model\n","Model=trainNN(data,neurons,epochs,learningRate,initEps=0.5,initType=\"xavier\",Analyse=True,errorTrace=50,plotTrace=50,seed=42)"]},{"cell_type":"markdown","metadata":{"id":"6_Iejs0Fdewk"},"source":["\n","### Further experiments\n","*--- This part is an optional homework. ---*\n","\n","*--- If you go for it and hand in a Jupyter notebook nicely documenting your experiments and findings, it is worth an extra badge.  ---*\n","\n","$\\color{red}{TASK:}$\n","\n","* Train a network initialized with zero (requires '*Initialization: 4 Methods*') and see what happens. Do you have an explanation for your observation?\n","* Analyse the impact of learning rate on your learning procedure.\n","* Experiment with different architectures and summarize your interesting findings.\n","* Take any of your datasets and multiply the input columns with a large value like 100. Plot the data again. \n","    * What happens to the decision boundary?\n","    * Train a neural network for it. Document your observation.\n","    "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"WPF-NN-orig.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}