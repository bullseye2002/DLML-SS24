# Natural Language Processing
## DG

<p>Schauen Sie sich die Grundlagen auf dem Weg zu ChatGPT und Co. an. Möchte man
neuronale Netze für Textverarbeitung nutzen, muss man Text als erstes in Zahlen
umwandeln, mit denen das neuronale Netz arbeiten kann. Dann kann man das neuronale
Netz „ganz normal“ für die gewünschte Anwendung trainieren.
Schauen Sie sich folgende Themen an:<p>

<ol>
    <li>Vorverarbeiten von Text:
        <ol>
            <li>Tokenization (teilt Text in Buchstaben/Wörter oder Tokens auf)</li>
            <li>Repräsentation von Buchstaben/Wörtern als Zahlen
                <ol>
                    <li>Word Embeddings (word2vec)</li>
                    <li>TF-IDF (term-frequency * inverse-document-frequency)</li>
                </ol>
            </li>
        </ol>
        <li>Trainieren eines neuronalen Netzes für eine Anwendung, bspw.:
        <ol>
            <li>Generieren von Text wie Shakespeare...</li>   
            <li>Sentiment Analyse von Text</li>
            <li>Neural Machine Translation</li>
        </ol>    
        </li>
        <li>Je nach Anwendung könnten Sie mit unterschiedlichen Netzarchitekturen experimentieren, bspw.:
            <ol>
                <li>Rekurrente neuronale Netze</li>
                <li>Transformer Architekturen</li>
            </ol>
        </li>
    <li>Optional: Stellen Sie Ihre 1-2 Lieblingstools vor, die in den letzten Monaten mit Hilfe
von Large Language Modellen entwickelt wurden</li>
</ol>


# Literatur:
* [Geron22], Kap. 16
* Jurafsky, Daniel, and James H. Martin. “Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.”, 3 rd edition, 2024, https://web.stanford.edu/~jurafsky/slp3/
